import torch 
import torch.nn as nn
import numpy as np
from simple_test import Dataset
from torch.optim import Adam
from tqdm import tqdm

torch.manual_seed(42)
np.random.seed(42)


n_datapoints = 1000
max_time = 5
n_timesteps = 10


inputs, extremals, trajectories = Dataset(n_datapoints, max_time,  n_timesteps)   #Dataset(n_datapoints, max_time,  n_timesteps)



# Basic Neural Network
class ProjectileModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ProjectileModel, self).__init__() 


        self.linear_relu_stack = nn.Sequential(
            nn.Linear(input_size, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, output_size),
        )

        """ self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size) """

    
    def forward(self, x):

        x = self.linear_relu_stack(x)

        """ x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x) """

        return x
    


#Hyperparamerets
learning_rate = 1e-4
batch_size = 32
epochs = 1000


#Network parameters
input_size = 4
output_size = 3   #When using only the extremals
hidden_layers = 120


#Make torch dataset
input_tensor = torch.from_numpy(inputs).float()
output_tensor = torch.from_numpy(extremals).float()
torch_dataset = torch.utils.data.TensorDataset(input_tensor, output_tensor)
train_data, test_data = torch.utils.data.random_split(torch_dataset, [int(0.8*n_datapoints), int(0.2*n_datapoints)])

train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle = True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)


#Model, Loss and Optimizer
model = ProjectileModel(input_size, hidden_layers, output_size)
loss_fn = nn.MSELoss()
optimizer = Adam(model.parameters(), lr=learning_rate)

with tqdm(range(epochs + 1), desc='Epochs', unit='epoch', leave=True, bar_format='{desc}: {percentage:3.0f}%|{bar}|{postfix}') as t:

    for epoch in t:

        model.train()
        running_loss = 0 
        for inputs, targets in train_loader:
            optimizer.zero_grad()

            # Forward pass
            pred = model(inputs) 
            loss = loss_fn(pred, targets)

            # Backward pass
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        t.set_postfix(Loss=f'{(running_loss/len(train_loader)):.6f}') 
        #t.set_postfix('Loss ': f{(running_loss/len(train_loader)):.6f})
        t.refresh() # Refresh to show postfix


        if epoch % 100 == 0:
            model.eval()
            test_loss = 0

            with torch.no_grad():
                for inputs, targets in test_loader:
                    pred = model(inputs) 
                    loss = loss_fn(pred, targets) 
                    test_loss += loss.item()

            print(f'Epoch {epoch+1}/{epochs} \nTraining Loss: {running_loss/len(train_loader):.4f} '
            f'\nTest Loss: {test_loss/len(test_loader):.6f}')
            print()


print("Finished training")



